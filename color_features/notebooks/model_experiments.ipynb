{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_experiments.ipynb\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(file_path):\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(file_path, header=None, skiprows=1)  # Skip the first row if it contains incorrect headers\n",
    "    \n",
    "    # Assign column names based on the structure of the file\n",
    "    data.columns = [\n",
    "        \"Hue\", \"Value\", \"Chroma\", \"r\", \"g\", \"b\", \n",
    "        \"L\", \"A\", \"B\"\n",
    "    ]\n",
    "    \n",
    "    # Inspect the data\n",
    "    print(data.head())\n",
    "    print(data.info())\n",
    "    \n",
    "    # Check for non-numeric values in the dataset\n",
    "    non_numeric_cols = data.select_dtypes(exclude=[np.number]).columns\n",
    "    if len(non_numeric_cols) > 0:\n",
    "        print(f\"Non-numeric columns found: {non_numeric_cols}\")\n",
    "        for col in non_numeric_cols:\n",
    "            print(f\"Unique values in {col}: {data[col].unique()}\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def clean_data(data):\n",
    "    # Check for non-numeric values in L, A, B columns\n",
    "    for col in [\"L\", \"A\", \"B\"]:\n",
    "        non_numeric_values = data[pd.to_numeric(data[col], errors='coerce').isnull()][col]\n",
    "        if not non_numeric_values.empty:\n",
    "            print(f\"Non-numeric values found in {col}: {non_numeric_values.unique()}\")\n",
    "            data = data[pd.to_numeric(data[col], errors='coerce').notnull()]\n",
    "    \n",
    "    # Convert L, A, B columns to numeric\n",
    "    data[\"L\"] = pd.to_numeric(data[\"L\"], errors='coerce')\n",
    "    data[\"A\"] = pd.to_numeric(data[\"A\"], errors='coerce')\n",
    "    data[\"B\"] = pd.to_numeric(data[\"B\"], errors='coerce')\n",
    "    \n",
    "    # Drop rows with NaN values in L, A, B columns\n",
    "    data = data.dropna(subset=[\"L\", \"A\", \"B\"])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # Extract input features (LAB values)\n",
    "    X = data[[\"L\", \"A\", \"B\"]].values.astype(np.float32)\n",
    "    \n",
    "    # Normalize L, A, B channels\n",
    "    X[:, 0] /= 100.0  # Normalize L channel: [0, 100] → [0, 1]\n",
    "    X[:, 1] = (X[:, 1] + 128) / 255.0  # Normalize A channel: [-128, 127] → [0, 1]\n",
    "    X[:, 2] = (X[:, 2] + 128) / 255.0  # Normalize B channel: [-128, 127] → [0, 1]\n",
    "    \n",
    "    # Extract target labels (Hue, Value, Chroma)\n",
    "    y_hue = pd.get_dummies(data[\"Hue\"]).values.astype(np.float32)  # One-hot encode Hue\n",
    "    y_value = data[\"Value\"].values.astype(np.float32).reshape(-1, 1)\n",
    "    y_chroma = data[\"Chroma\"].values.astype(np.float32).reshape(-1, 1)\n",
    "    \n",
    "    return X, y_hue, y_value, y_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_model(input_shape, num_hues):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    hue_output = Dense(num_hues, activation='softmax', name=\"hue_output\")(x)\n",
    "    value_output = Dense(1, activation='linear', name=\"value_output\")(x)\n",
    "    chroma_output = Dense(1, activation='linear', name=\"chroma_output\")(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=[hue_output, value_output, chroma_output])\n",
    "\n",
    "# Implement simplified quantization without tfmot\n",
    "def apply_quantization(model):\n",
    "    # Using TensorFlow Lite's quantization directly\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    quantized_tflite_model = converter.convert()\n",
    "    \n",
    "    # Save the quantized model to a temporary file and reload\n",
    "    with open('temp_quantized_model.tflite', 'wb') as f:\n",
    "        f.write(quantized_tflite_model)\n",
    "    \n",
    "    # Note: Returning the original model since we can't directly convert TFLite back to Keras\n",
    "    # The quantized model will be used during save_model\n",
    "    print(\"Model quantized and saved as 'temp_quantized_model.tflite'\")\n",
    "    return model\n",
    "\n",
    "# Implement pruning without tfmot\n",
    "def apply_pruning(model):\n",
    "    # Simple magnitude-based pruning implementation\n",
    "    # Create a new model with the same architecture but apply masks to small weights\n",
    "    pruned_model = tf.keras.models.clone_model(model)\n",
    "    pruned_model.set_weights(model.get_weights())\n",
    "    \n",
    "    # Define sparsity level (percentage of weights to be zeroed)\n",
    "    sparsity = 0.5  # 50% sparsity\n",
    "    \n",
    "    # Apply pruning to each layer with weights\n",
    "    for layer in pruned_model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Dense):\n",
    "            weights = layer.get_weights()\n",
    "            if len(weights) > 0:  # Check if layer has weights\n",
    "                # Apply pruning to weights (not biases)\n",
    "                w = weights[0]\n",
    "                # Get threshold value for the specified sparsity\n",
    "                threshold = tf.sort(tf.abs(tf.reshape(w, [-1])))[int(w.size * sparsity)]\n",
    "                # Create a mask that zeros out weights below threshold\n",
    "                mask = tf.cast(tf.abs(w) >= threshold, w.dtype)\n",
    "                # Apply mask\n",
    "                weights[0] = w * mask\n",
    "                layer.set_weights(weights)\n",
    "    \n",
    "    print(f\"Model pruned with {sparsity*100}% sparsity\")\n",
    "    return pruned_model\n",
    "\n",
    "def save_model(model, file_name):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print(f\"Model saved as {file_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "   Hue  Value  Chroma         r         g         b         L         A  \\\n",
      "0  10B    1.0       1  0.069966  0.102321  0.125645  8.766975 -1.873666   \n",
      "1  10B    1.0       2  0.047615  0.104012  0.144154  8.752576 -2.435589   \n",
      "2  10B    1.0       3  0.012557  0.106032  0.164007  8.734874 -3.121113   \n",
      "3  10B    1.0       4  0.000000  0.108210  0.184417  9.105021 -1.903252   \n",
      "4  10B    1.0       5  0.000000  0.110270  0.204285  9.653357  0.194611   \n",
      "\n",
      "           B  \n",
      "0  -5.355077  \n",
      "1  -8.942874  \n",
      "2 -12.765529  \n",
      "3 -16.018042  \n",
      "4 -18.828227  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10447 entries, 0 to 10446\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Hue     10447 non-null  object \n",
      " 1   Value   10447 non-null  float64\n",
      " 2   Chroma  10447 non-null  int64  \n",
      " 3   r       10447 non-null  float64\n",
      " 4   g       10447 non-null  float64\n",
      " 5   b       10447 non-null  float64\n",
      " 6   L       10447 non-null  float64\n",
      " 7   A       10447 non-null  float64\n",
      " 8   B       10447 non-null  float64\n",
      "dtypes: float64(7), int64(1), object(1)\n",
      "memory usage: 734.7+ KB\n",
      "None\n",
      "Non-numeric columns found: Index(['Hue'], dtype='object')\n",
      "Unique values in Hue: ['10B' '10BG' '10G' '10GY' '10P' '10PB' '10R' '10RP' '10Y' '10YR' '2.5B'\n",
      " '2.5BG' '2.5G' '2.5GY' '2.5P' '2.5PB' '2.5R' '2.5RP' '2.5Y' '2.5YR' '5B'\n",
      " '5BG' '5G' '5GY' '5P' '5PB' '5R' '5RP' '5Y' '5YR' '7.5B' '7.5BG' '7.5G'\n",
      " '7.5GY' '7.5P' '7.5PB' '7.5R' '7.5RP' '7.5Y' '7.5YR' 'N']\n",
      "Cleaning data...\n",
      "Preprocessing data...\n",
      "Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742781926.500903   88295 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3539 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742781929.532176   90856 service.cc:152] XLA service 0x7fc9dc0030d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1742781929.532367   90856 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
      "2025-03-24 07:35:29.606032: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1742781929.897846   90856 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 29/262\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - chroma_output_loss: 243.5101 - chroma_output_mae: 12.8940 - hue_output_accuracy: 0.0321 - hue_output_loss: 4.4414 - loss: 283.9442 - value_output_loss: 35.9927 - value_output_mae: 5.5189"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742781932.343203   90856 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - chroma_output_loss: 188.3340 - chroma_output_mae: 11.6899 - hue_output_accuracy: 0.0560 - hue_output_loss: 3.9229 - loss: 223.0089 - value_output_loss: 30.7497 - value_output_mae: 5.2593 - val_chroma_output_loss: 141.8045 - val_chroma_output_mae: 8.8013 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 4.8048 - val_loss: 162.6631 - val_value_output_loss: 14.8076 - val_value_output_mae: 3.2090\n",
      "Epoch 2/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - chroma_output_loss: 55.3378 - chroma_output_mae: 5.8253 - hue_output_accuracy: 0.1332 - hue_output_loss: 2.9118 - loss: 65.5018 - value_output_loss: 7.2513 - value_output_mae: 2.2248 - val_chroma_output_loss: 35.0288 - val_chroma_output_mae: 4.1428 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 6.4797 - val_loss: 43.5760 - val_value_output_loss: 2.0284 - val_value_output_mae: 1.2209\n",
      "Epoch 3/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - chroma_output_loss: 24.8356 - chroma_output_mae: 3.8084 - hue_output_accuracy: 0.1762 - hue_output_loss: 2.5532 - loss: 30.2287 - value_output_loss: 2.8412 - value_output_mae: 1.3136 - val_chroma_output_loss: 11.3444 - val_chroma_output_mae: 2.4860 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 7.9144 - val_loss: 20.0322 - val_value_output_loss: 0.8205 - val_value_output_mae: 0.7101\n",
      "Epoch 4/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - chroma_output_loss: 24.3885 - chroma_output_mae: 3.7586 - hue_output_accuracy: 0.2091 - hue_output_loss: 2.3809 - loss: 29.1465 - value_output_loss: 2.3783 - value_output_mae: 1.2113 - val_chroma_output_loss: 12.0363 - val_chroma_output_mae: 2.4355 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 8.8228 - val_loss: 21.3719 - val_value_output_loss: 0.4895 - val_value_output_mae: 0.5153\n",
      "Epoch 5/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - chroma_output_loss: 23.5474 - chroma_output_mae: 3.6692 - hue_output_accuracy: 0.2144 - hue_output_loss: 2.2970 - loss: 27.8897 - value_output_loss: 2.0455 - value_output_mae: 1.1222 - val_chroma_output_loss: 12.5497 - val_chroma_output_mae: 2.7062 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 9.4349 - val_loss: 22.4161 - val_value_output_loss: 0.4911 - val_value_output_mae: 0.5141\n",
      "Epoch 6/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - chroma_output_loss: 21.7792 - chroma_output_mae: 3.5485 - hue_output_accuracy: 0.2328 - hue_output_loss: 2.2450 - loss: 25.9629 - value_output_loss: 1.9397 - value_output_mae: 1.0872 - val_chroma_output_loss: 11.3242 - val_chroma_output_mae: 2.5584 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 9.9514 - val_loss: 21.7957 - val_value_output_loss: 0.4915 - val_value_output_mae: 0.5090\n",
      "Epoch 7/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - chroma_output_loss: 21.7029 - chroma_output_mae: 3.5086 - hue_output_accuracy: 0.2325 - hue_output_loss: 2.1876 - loss: 25.7452 - value_output_loss: 1.8552 - value_output_mae: 1.0625 - val_chroma_output_loss: 11.8863 - val_chroma_output_mae: 2.4236 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 10.3611 - val_loss: 22.9098 - val_value_output_loss: 0.6218 - val_value_output_mae: 0.6118\n",
      "Epoch 8/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - chroma_output_loss: 21.9863 - chroma_output_mae: 3.5606 - hue_output_accuracy: 0.2436 - hue_output_loss: 2.1621 - loss: 26.0237 - value_output_loss: 1.8758 - value_output_mae: 1.0724 - val_chroma_output_loss: 11.0457 - val_chroma_output_mae: 2.4997 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 10.6650 - val_loss: 22.1462 - val_value_output_loss: 0.4031 - val_value_output_mae: 0.4538\n",
      "Epoch 9/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - chroma_output_loss: 21.5702 - chroma_output_mae: 3.4695 - hue_output_accuracy: 0.2471 - hue_output_loss: 2.1347 - loss: 25.3441 - value_output_loss: 1.6397 - value_output_mae: 0.9980 - val_chroma_output_loss: 10.0851 - val_chroma_output_mae: 2.2949 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 10.9899 - val_loss: 21.5251 - val_value_output_loss: 0.4222 - val_value_output_mae: 0.4583\n",
      "Epoch 10/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - chroma_output_loss: 21.2651 - chroma_output_mae: 3.4302 - hue_output_accuracy: 0.2530 - hue_output_loss: 2.1148 - loss: 25.0530 - value_output_loss: 1.6730 - value_output_mae: 0.9960 - val_chroma_output_loss: 9.9366 - val_chroma_output_mae: 2.2557 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 11.2233 - val_loss: 21.9369 - val_value_output_loss: 0.7336 - val_value_output_mae: 0.6996\n",
      "Epoch 11/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - chroma_output_loss: 20.2730 - chroma_output_mae: 3.3628 - hue_output_accuracy: 0.2518 - hue_output_loss: 2.1065 - loss: 23.9325 - value_output_loss: 1.5537 - value_output_mae: 0.9691 - val_chroma_output_loss: 10.1480 - val_chroma_output_mae: 2.2097 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 11.5766 - val_loss: 22.0683 - val_value_output_loss: 0.3174 - val_value_output_mae: 0.3671\n",
      "Epoch 12/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - chroma_output_loss: 21.2721 - chroma_output_mae: 3.4363 - hue_output_accuracy: 0.2693 - hue_output_loss: 2.0855 - loss: 24.9118 - value_output_loss: 1.5542 - value_output_mae: 0.9730 - val_chroma_output_loss: 9.9232 - val_chroma_output_mae: 2.2722 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 11.8380 - val_loss: 22.1548 - val_value_output_loss: 0.3535 - val_value_output_mae: 0.3924\n",
      "Epoch 13/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - chroma_output_loss: 20.0572 - chroma_output_mae: 3.3421 - hue_output_accuracy: 0.2693 - hue_output_loss: 2.0816 - loss: 23.7704 - value_output_loss: 1.6319 - value_output_mae: 0.9941 - val_chroma_output_loss: 11.0783 - val_chroma_output_mae: 2.3350 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 12.0830 - val_loss: 23.5432 - val_value_output_loss: 0.3334 - val_value_output_mae: 0.3663\n",
      "Epoch 14/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - chroma_output_loss: 20.7253 - chroma_output_mae: 3.4125 - hue_output_accuracy: 0.2606 - hue_output_loss: 2.1071 - loss: 24.3020 - value_output_loss: 1.4707 - value_output_mae: 0.9478 - val_chroma_output_loss: 10.6143 - val_chroma_output_mae: 2.3882 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 12.2467 - val_loss: 23.2937 - val_value_output_loss: 0.3876 - val_value_output_mae: 0.4186\n",
      "Epoch 15/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - chroma_output_loss: 20.0194 - chroma_output_mae: 3.3594 - hue_output_accuracy: 0.2493 - hue_output_loss: 2.1413 - loss: 23.6618 - value_output_loss: 1.5030 - value_output_mae: 0.9627 - val_chroma_output_loss: 11.3147 - val_chroma_output_mae: 2.4737 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 12.5008 - val_loss: 24.2625 - val_value_output_loss: 0.4054 - val_value_output_mae: 0.4379\n",
      "Epoch 16/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - chroma_output_loss: 20.5503 - chroma_output_mae: 3.4074 - hue_output_accuracy: 0.2594 - hue_output_loss: 2.0688 - loss: 24.0535 - value_output_loss: 1.4344 - value_output_mae: 0.9325 - val_chroma_output_loss: 10.0874 - val_chroma_output_mae: 2.2150 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 12.7105 - val_loss: 23.1582 - val_value_output_loss: 0.3086 - val_value_output_mae: 0.3510\n",
      "Epoch 17/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - chroma_output_loss: 19.7521 - chroma_output_mae: 3.3110 - hue_output_accuracy: 0.2804 - hue_output_loss: 2.0238 - loss: 23.1969 - value_output_loss: 1.4217 - value_output_mae: 0.9231 - val_chroma_output_loss: 10.0787 - val_chroma_output_mae: 2.2341 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 12.9973 - val_loss: 23.4456 - val_value_output_loss: 0.3050 - val_value_output_mae: 0.3578\n",
      "Epoch 18/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - chroma_output_loss: 20.2961 - chroma_output_mae: 3.3567 - hue_output_accuracy: 0.2794 - hue_output_loss: 2.0411 - loss: 23.7240 - value_output_loss: 1.3865 - value_output_mae: 0.9164 - val_chroma_output_loss: 11.8292 - val_chroma_output_mae: 2.4427 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 12.9502 - val_loss: 25.2728 - val_value_output_loss: 0.4426 - val_value_output_mae: 0.4801\n",
      "Epoch 19/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - chroma_output_loss: 20.7409 - chroma_output_mae: 3.4264 - hue_output_accuracy: 0.2743 - hue_output_loss: 2.0643 - loss: 24.2259 - value_output_loss: 1.4220 - value_output_mae: 0.9293 - val_chroma_output_loss: 10.2412 - val_chroma_output_mae: 2.3148 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 13.5964 - val_loss: 24.2068 - val_value_output_loss: 0.3088 - val_value_output_mae: 0.3616\n",
      "Epoch 20/20\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - chroma_output_loss: 20.3679 - chroma_output_mae: 3.3366 - hue_output_accuracy: 0.2629 - hue_output_loss: 2.0543 - loss: 23.8151 - value_output_loss: 1.3940 - value_output_mae: 0.9140 - val_chroma_output_loss: 9.6442 - val_chroma_output_mae: 2.2107 - val_hue_output_accuracy: 0.0000e+00 - val_hue_output_loss: 13.5414 - val_loss: 23.5303 - val_value_output_loss: 0.2869 - val_value_output_mae: 0.3387\n",
      "Applying pruning...\n",
      "Model pruned with 50.0% sparsity\n",
      "Applying quantization...\n",
      "INFO:tensorflow:Assets written to: /tmp/tmphe41_qlt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmphe41_qlt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmphe41_qlt'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 3), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  List[TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)]\n",
      "Captures:\n",
      "  140509253851024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253847568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253846416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253849872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253850832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253847760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253850640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253856976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253859472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253849488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253846224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253860048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253848912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253858896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253855056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253852368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253851792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509189835088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1742781980.892541   88295 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1742781980.892592   88295 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-03-24 07:36:20.893043: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmphe41_qlt\n",
      "2025-03-24 07:36:20.894262: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-24 07:36:20.894291: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmphe41_qlt\n",
      "I0000 00:00:1742781980.902085   88295 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n",
      "2025-03-24 07:36:20.903307: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-24 07:36:20.942112: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmphe41_qlt\n",
      "2025-03-24 07:36:20.954014: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 60977 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model quantized and saved as 'temp_quantized_model.tflite'\n",
      "Saving optimized model...\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpp__a27hj/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpp__a27hj/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpp__a27hj'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 3), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  List[TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)]\n",
      "Captures:\n",
      "  140509253851024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253847568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253846416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253849872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253850832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253847760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253850640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253856976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253859472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253849488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253846224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253860048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253848912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253858896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253855056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253852368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509253851792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140509189835088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1742781981.517095   88295 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1742781981.517168   88295 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as optimized_color_model.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 07:36:21.517399: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpp__a27hj\n",
      "2025-03-24 07:36:21.518121: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-24 07:36:21.518132: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpp__a27hj\n",
      "2025-03-24 07:36:21.525362: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-24 07:36:21.561910: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpp__a27hj\n",
      "2025-03-24 07:36:21.573493: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 56100 microseconds.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Step 1: Load the dataset\n",
    "    file_path = \"/home/sala/data/equivalent_munsell.csv\"  # Replace with the actual path to your dataset\n",
    "    print(\"Loading data...\")\n",
    "    data = load_data(file_path)\n",
    "    \n",
    "    # Step 2: Clean the dataset\n",
    "    print(\"Cleaning data...\")\n",
    "    data = clean_data(data)\n",
    "    \n",
    "    # Step 3: Preprocess the dataset\n",
    "    print(\"Preprocessing data...\")\n",
    "    X, y_hue, y_value, y_chroma = preprocess_data(data)\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "    y_hue_train, y_hue_val = y_hue[:split_idx], y_hue[split_idx:]\n",
    "    y_value_train, y_value_val = y_value[:split_idx], y_value[split_idx:]\n",
    "    y_chroma_train, y_chroma_val = y_chroma[:split_idx], y_chroma[split_idx:]\n",
    "    \n",
    "    # Step 4: Build the model\n",
    "    num_hues = y_hue.shape[1]  # Number of unique hues (one-hot encoded)\n",
    "    input_shape = (3,)  # LAB channels\n",
    "    print(\"Building model...\")\n",
    "    model = build_model(input_shape, num_hues)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={\n",
    "            \"hue_output\": \"categorical_crossentropy\",\n",
    "            \"value_output\": \"mse\",\n",
    "            \"chroma_output\": \"mse\"\n",
    "        },\n",
    "        metrics={\n",
    "            \"hue_output\": \"accuracy\",\n",
    "            \"value_output\": \"mae\",\n",
    "            \"chroma_output\": \"mae\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Step 5: Train the model\n",
    "    print(\"Training model...\")\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        {\"hue_output\": y_hue_train, \"value_output\": y_value_train, \"chroma_output\": y_chroma_train},\n",
    "        validation_data=(X_val, {\"hue_output\": y_hue_val, \"value_output\": y_value_val, \"chroma_output\": y_chroma_val}),\n",
    "        epochs=20,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Step 6: Apply pruning\n",
    "    print(\"Applying pruning...\")\n",
    "    pruned_model = apply_pruning(model)\n",
    "    \n",
    "    # Step 7: Apply quantization\n",
    "    print(\"Applying quantization...\")\n",
    "    quantized_model = apply_quantization(pruned_model)\n",
    "    \n",
    "    # Step 8: Save the optimized model\n",
    "    print(\"Saving optimized model...\")\n",
    "    save_model(quantized_model, \"optimized_color_model.tflite\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Hue  Value  Chroma         r         g         b         L         A  \\\n",
      "0  10B    1.0       1  0.069966  0.102321  0.125645  8.766975 -1.873666   \n",
      "1  10B    1.0       2  0.047615  0.104012  0.144154  8.752576 -2.435589   \n",
      "2  10B    1.0       3  0.012557  0.106032  0.164007  8.734874 -3.121113   \n",
      "3  10B    1.0       4  0.000000  0.108210  0.184417  9.105021 -1.903252   \n",
      "4  10B    1.0       5  0.000000  0.110270  0.204285  9.653357  0.194611   \n",
      "\n",
      "           B  \n",
      "0  -5.355077  \n",
      "1  -8.942874  \n",
      "2 -12.765529  \n",
      "3 -16.018042  \n",
      "4 -18.828227  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10447 entries, 0 to 10446\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Hue     10447 non-null  object \n",
      " 1   Value   10447 non-null  float64\n",
      " 2   Chroma  10447 non-null  int64  \n",
      " 3   r       10447 non-null  float64\n",
      " 4   g       10447 non-null  float64\n",
      " 5   b       10447 non-null  float64\n",
      " 6   L       10447 non-null  float64\n",
      " 7   A       10447 non-null  float64\n",
      " 8   B       10447 non-null  float64\n",
      "dtypes: float64(7), int64(1), object(1)\n",
      "memory usage: 734.7+ KB\n",
      "None\n",
      "Non-numeric columns found: Index(['Hue'], dtype='object')\n",
      "Unique values in Hue: ['10B' '10BG' '10G' '10GY' '10P' '10PB' '10R' '10RP' '10Y' '10YR' '2.5B'\n",
      " '2.5BG' '2.5G' '2.5GY' '2.5P' '2.5PB' '2.5R' '2.5RP' '2.5Y' '2.5YR' '5B'\n",
      " '5BG' '5G' '5GY' '5P' '5PB' '5R' '5RP' '5Y' '5YR' '7.5B' '7.5BG' '7.5G'\n",
      " '7.5GY' '7.5P' '7.5PB' '7.5R' '7.5RP' '7.5Y' '7.5YR' 'N']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sala/miniconda3/lib/python3.12/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m     evaluate_model(predictions, y_hue_test, y_value_test, y_chroma_test)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 77\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m predictions \u001b[38;5;241m=\u001b[39m run_inference(interpreter, X_test)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hue_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_value_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_chroma_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 46\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(predictions, y_hue_test, y_value_test, y_chroma_test)\u001b[0m\n\u001b[1;32m     43\u001b[0m pred_chroma \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([p[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchroma\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m predictions])\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Compute metrics\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m hue_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m(np\u001b[38;5;241m.\u001b[39margmax(y_hue_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), pred_hue)\n\u001b[1;32m     47\u001b[0m value_mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_value_test, pred_value)\n\u001b[1;32m     48\u001b[0m chroma_mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_chroma_test, pred_chroma)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 3: Load the trained model\n",
    "def load_model(model_path):\n",
    "    # Load the TFLite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    return interpreter\n",
    "\n",
    "# Step 4: Run inference on the test data\n",
    "def run_inference(interpreter, X_test):\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        # Prepare input data\n",
    "        input_data = np.expand_dims(X_test[i], axis=0).astype(input_details[0]['dtype'])\n",
    "        \n",
    "        # Set input tensor\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        \n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "        \n",
    "        # Get output tensors\n",
    "        hue_output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        value_output = interpreter.get_tensor(output_details[1]['index'])\n",
    "        chroma_output = interpreter.get_tensor(output_details[2]['index'])\n",
    "        \n",
    "        # Append predictions\n",
    "        predictions.append({\n",
    "            \"hue\": np.argmax(hue_output),  # Predicted class for Hue\n",
    "            \"value\": value_output[0][0],   # Predicted Value\n",
    "            \"chroma\": chroma_output[0][0]  # Predicted Chroma\n",
    "        })\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "def evaluate_model(predictions, y_hue_test, y_value_test, y_chroma_test):\n",
    "    # Extract predictions\n",
    "    pred_hue = np.array([p[\"hue\"] for p in predictions])\n",
    "    pred_value = np.array([p[\"value\"] for p in predictions])\n",
    "    pred_chroma = np.array([p[\"chroma\"] for p in predictions])\n",
    "    \n",
    "    # Compute metrics\n",
    "    hue_accuracy = accuracy_score(np.argmax(y_hue_test, axis=1), pred_hue)\n",
    "    value_mae = mean_absolute_error(y_value_test, pred_value)\n",
    "    chroma_mae = mean_absolute_error(y_chroma_test, pred_chroma)\n",
    "    \n",
    "    print(f\"Hue Accuracy: {hue_accuracy * 100:.2f}%\")\n",
    "    print(f\"Value MAE: {value_mae:.4f}\")\n",
    "    print(f\"Chroma MAE: {chroma_mae:.4f}\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Paths\n",
    "    file_path = \"/home/sala/data/equivalent_munsell.csv\"\n",
    "    model_path = \"/home/sala/iit/dsgp/soil_analysis_automation_system/color_features/notebooks/optimized_color_model.tflite\"  \n",
    "    # Load and preprocess the dataset\n",
    "    data = load_data(file_path)\n",
    "    X, y_hue, y_value, y_chroma = preprocess_data(data)\n",
    "    \n",
    "    # Split into training and test sets\n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_hue_train, y_hue_test = y_hue[:split_idx], y_hue[split_idx:]\n",
    "    y_value_train, y_value_test = y_value[:split_idx], y_value[split_idx:]\n",
    "    y_chroma_train, y_chroma_test = y_chroma[:split_idx], y_chroma[split_idx:]\n",
    "    \n",
    "    # Load the trained model\n",
    "    interpreter = load_model(model_path)\n",
    "    \n",
    "    # Run inference\n",
    "    predictions = run_inference(interpreter, X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    evaluate_model(predictions, y_hue_test, y_value_test, y_chroma_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Different Model Architectures\n",
    "def simple_cnn(input_shape=(224, 224, 3), num_classes=5):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with Hyperparameters\n",
    "def train_model(model, train_ds, val_ds, epochs=50, learning_rate=3e-4):\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        'models/checkpoints/best_model.h5',\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint, early_stopping]\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory data/soil/labeled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load Data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/soil/labeled\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmap(build_data_pipeline(), num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[1;32m     12\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/soil/labeled\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m     image_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     20\u001b[0m )\u001b[38;5;241m.\u001b[39mmap(build_data_pipeline(), num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Experiment 1: Simple CNN\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/keras/src/utils/image_dataset_utils.py:232\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[0;32m--> 232\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALLOWLIST_FORMATS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/keras/src/utils/dataset_utils.py:530\u001b[0m, in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    529\u001b[0m     subdirs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    532\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/lib/io/file_io.py:768\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03mThe list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m  errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(path):\n\u001b[0;32m--> 768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[1;32m    769\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    770\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    771\u001b[0m       message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find directory \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[1;32m    773\u001b[0m \u001b[38;5;66;03m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    776\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str_any(filename)\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mGetChildren(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[1;32m    778\u001b[0m ]\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory data/soil/labeled"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    'data/soil/labeled',\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    label_mode='int',\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=42\n",
    ").map(build_data_pipeline(), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    'data/soil/labeled',\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    label_mode='int',\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=42\n",
    ").map(build_data_pipeline(), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Experiment 1: Simple CNN\n",
    "simple_cnn_model = simple_cnn()\n",
    "history_simple_cnn = train_model(simple_cnn_model, train_ds, val_ds, epochs=10)\n",
    "\n",
    "# Experiment 2: MobileNetV3-LAB\n",
    "mobile_net_model = mobile_soil_classifier(num_classes=5)\n",
    "history_mobile_net = train_model(mobile_net_model, train_ds, val_ds, epochs=10)\n",
    "\n",
    "# Plot Training History\n",
    "def plot_training_history(history, model_name):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title(f'{model_name} Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'{model_name} Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history_simple_cnn, 'Simple CNN')\n",
    "plot_training_history(history_mobile_net, 'MobileNetV3-LAB')\n",
    "\n",
    "# Try Different Training Strategies\n",
    "# Experiment 3: MobileNetV3-LAB with Quantization and Pruning\n",
    "optimized_model = optimize_for_mobile(mobile_net_model)\n",
    "history_optimized = train_model(optimized_model, train_ds, val_ds, epochs=10)\n",
    "\n",
    "# Evaluate Model Performance\n",
    "def evaluate_model(model, val_ds):\n",
    "    loss, accuracy = model.evaluate(val_ds)\n",
    "    print(f'Validation Loss: {loss}')\n",
    "    print(f'Validation Accuracy: {accuracy}')\n",
    "\n",
    "evaluate_model(optimized_model, val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimize_for_mobile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Try Different Training Strategies\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Experiment 3: MobileNetV3-LAB with Quantization and Pruning\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m optimized_model \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_for_mobile\u001b[49m(mobile_net_model)\n\u001b[1;32m      4\u001b[0m history_optimized \u001b[38;5;241m=\u001b[39m train_model(optimized_model, train_ds, val_ds, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Evaluate Model Performance\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimize_for_mobile' is not defined"
     ]
    }
   ],
   "source": [
    "# Try Different Training Strategies\n",
    "# Experiment 3: MobileNetV3-LAB with Quantization and Pruning\n",
    "optimized_model = optimize_for_mobile(mobile_net_model)\n",
    "history_optimized = train_model(optimized_model, train_ds, val_ds, epochs=10)\n",
    "\n",
    "# Evaluate Model Performance\n",
    "def evaluate_model(model, val_ds):\n",
    "    loss, accuracy = model.evaluate(val_ds)\n",
    "    print(f'Validation Loss: {loss}')\n",
    "    print(f'Validation Accuracy: {accuracy}')\n",
    "\n",
    "evaluate_model(optimized_model, val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
