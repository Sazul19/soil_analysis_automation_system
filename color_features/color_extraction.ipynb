{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable GPU acceleration and mixed precision\n",
    "tf.config.optimizer.set_jit(True)\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TARGET_SIZE = (224, 224)\n",
    "BATCH_SIZE = 256\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Phase 1: Optimized General Color Model ###\n",
    "class LABPreprocessing(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mean = tf.constant([0.485, 0.456, 0.406])\n",
    "        self.std = tf.constant([0.229, 0.224, 0.225])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Normalize with ImageNet stats\n",
    "        x = (inputs - self.mean) / self.std\n",
    "        \n",
    "        # Convert RGB to LAB using TF ops\n",
    "        xyz = tf.image.rgb_to_xyz(x)\n",
    "        xyz = tf.clip_by_value(xyz, 1e-8, 1.0)\n",
    "        \n",
    "        # XYZ to LAB conversion\n",
    "        xyz = tf.unstack(xyz, axis=-1)\n",
    "        x, y, z = [t * 100.0 for t in xyz]\n",
    "        \n",
    "        l = tf.clip(116.0 * self._f(y/100.0) - 16.0, 0, 100)\n",
    "        a = 500.0 * (self._f(x/95.047) - self._f(y/100.0))\n",
    "        b = 200.0 * (self._f(y/100.0) - self._f(z/108.883))\n",
    "        \n",
    "        # Dynamic range adjustment\n",
    "        l = l / 100.0\n",
    "        a = (a + 128.0) / 255.0\n",
    "        b = (b + 128.0) / 255.0\n",
    "        \n",
    "        return tf.stack([l, a, b], axis=-1)\n",
    "    \n",
    "    def _f(self, t):\n",
    "        return tf.where(t > 0.008856, tf.pow(t, 1/3), 7.787*t + 16/116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimized_model():\n",
    "    inputs = layers.Input((*TARGET_SIZE, 3), dtype=tf.float32)\n",
    "    x = layers.Rescaling(1./255)(inputs)\n",
    "    \n",
    "    # Enhanced preprocessing\n",
    "    x = LABPreprocessing()(x)\n",
    "    \n",
    "    # Architecture optimized for mobile\n",
    "    base = EfficientNetB3(include_top=False, weights='imagenet', \n",
    "                         input_shape=(*TARGET_SIZE, 3))\n",
    "    base.trainable = False\n",
    "    \n",
    "    # Feature extraction with depthwise pooling\n",
    "    x = base(x)\n",
    "    x = layers.DepthwiseConv2D(3, activation='swish')(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Mixed precision-aware dense layers\n",
    "    x = layers.Dense(256, dtype='float32')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    outputs = layers.Dense(3, activation='sigmoid', dtype='float32')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                 loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Phase 2: Optimized Self-Supervised Learning ###\n",
    "class SimCLR(tf.keras.Model):\n",
    "    def __init__(self, base_model, temperature=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.encoder = base_model\n",
    "        self.projection = tf.keras.Sequential([\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.LayerNormalization(),\n",
    "            layers.Dense(128)\n",
    "        ])\n",
    "        \n",
    "    def compile(self, optimizer, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        \n",
    "    def train_step(self, images):\n",
    "        # Generate augmented views\n",
    "        aug1 = self.augment(images)\n",
    "        aug2 = self.augment(images)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            z1 = self.projection(self.encoder(aug1))\n",
    "            z2 = self.projection(self.encoder(aug2))\n",
    "            loss = self.nt_xent_loss(z1, z2)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "    \n",
    "    def augment(self, x):\n",
    "        return tf.keras.Sequential([\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(0.2),\n",
    "            layers.RandomZoom(0.2),\n",
    "            layers.RandomContrast(0.2),\n",
    "            layers.GaussianNoise(0.1)\n",
    "        ])(x)\n",
    "    \n",
    "    def nt_xent_loss(self, z1, z2):\n",
    "        z = tf.concat([z1, z2], axis=0)\n",
    "        similarity = tf.matmul(z, z, transpose_b=True) / self.temperature\n",
    "        batch_size = tf.shape(z1)[0]\n",
    "        \n",
    "        # Create positive mask\n",
    "        pos_mask = tf.eye(batch_size * 2, batch_size * 2, dtype=tf.bool)\n",
    "        pos_mask = tf.math.logical_xor(pos_mask, tf.roll(pos_mask, batch_size, axis=0))\n",
    "        \n",
    "        # Calculate NT-Xent loss\n",
    "        logits = tf.where(pos_mask, similarity, -1e9)\n",
    "        labels = tf.range(batch_size)\n",
    "        labels = tf.concat([labels, labels], axis=0)\n",
    "        return tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Phase 3: Vectorized Munsell Mapping ###\n",
    "class MunsellMapper:\n",
    "    def __init__(self, csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.lab = df[[\"L\", \"a\", \"b\"]].values\n",
    "        self.codes = df[\"munsell\"].values\n",
    "        self.nn = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(self.lab)\n",
    "        \n",
    "    def lab_to_munsell(self, lab):\n",
    "        lab = np.clip(lab, [0, -128, -128], [100, 127, 127])\n",
    "        distances, indices = self.nn.kneighbors(lab.reshape(1, -1))\n",
    "        \n",
    "        # Apply tolerances (Â±1 unit for value/chroma)\n",
    "        candidates = self.codes[indices[0]]\n",
    "        lab_values = self.lab[indices[0]]\n",
    "        \n",
    "        # Find best match within tolerance\n",
    "        valid = (np.abs(lab_values[:, 0] - lab[0]) <= 1) & \\\n",
    "                (np.abs(lab_values[:, 1] - lab[1]) <= 1)\n",
    "        if np.any(valid):\n",
    "            return candidates[valid][0]\n",
    "        return candidates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Phase 4: Mobile-Optimized Deployment ###\n",
    "class MobileInference:\n",
    "    def __init__(self, model_path, munsell_mapper):\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "        self.interpreter.allocate_tensors()\n",
    "        self.mapper = munsell_mapper\n",
    "        \n",
    "    def predict(self, images):\n",
    "        input_details = self.interpreter.get_input_details()\n",
    "        output_details = self.interpreter.get_output_details()\n",
    "        \n",
    "        # Process multiple images\n",
    "        outputs = []\n",
    "        for img in images:\n",
    "            img = tf.image.resize(img, TARGET_SIZE).numpy()\n",
    "            self.interpreter.set_tensor(input_details[0]['index'], img[np.newaxis])\n",
    "            self.interpreter.invoke()\n",
    "            lab = self.interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "            \n",
    "            # Denormalize LAB\n",
    "            lab = [\n",
    "                lab[0] * 100,\n",
    "                (lab[1] * 255) - 128,\n",
    "                (lab[2] * 255) - 128\n",
    "            ]\n",
    "            outputs.append(self.mapper.lab_to_munsell(lab))\n",
    "        \n",
    "        # Return most frequent prediction\n",
    "        return max(set(outputs), key=outputs.count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimized Data Pipeline ###\n",
    "def create_pipeline(file_pattern, shuffle=True):\n",
    "    def process_path(file_path):\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        return tf.image.convert_image_dtype(img, tf.float32)\n",
    "    \n",
    "    ds = tf.data.Dataset.list_files(file_pattern)\n",
    "    if shuffle: ds = ds.shuffle(10000)\n",
    "    return ds.map(process_path, num_parallel_calls=AUTOTUNE) \\\n",
    "             .batch(BATCH_SIZE) \\\n",
    "             .prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enhanced Evaluation ###\n",
    "class ColorEvaluator:\n",
    "    def __init__(self, model, mapper):\n",
    "        self.model = model\n",
    "        self.mapper = mapper\n",
    "        \n",
    "    def delta_e(self, lab1, lab2):\n",
    "        # CIEDE2000 implementation\n",
    "        try:\n",
    "            from colormath.color_objects import LabColor\n",
    "            from colormath.color_diff import delta_e_cie2000\n",
    "            return delta_e_cie2000(LabColor(*lab1), LabColor(*lab2))\n",
    "        except ImportError:\n",
    "            # Fallback to Euclidean distance\n",
    "            return np.sqrt(np.sum((lab1 - lab2)**2))\n",
    "        \n",
    "    def evaluate(self, dataset):\n",
    "        lab_errors = []\n",
    "        delta_es = []\n",
    "        munsell_acc = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in dataset:\n",
    "            preds = self.model.predict(batch)\n",
    "            for i in range(len(batch)):\n",
    "                # Convert predictions to LAB\n",
    "                lab_pred = [\n",
    "                    preds[i][0] * 100,\n",
    "                    (preds[i][1] * 255) - 128,\n",
    "                    (preds[i][2] * 255) - 128\n",
    "                ]\n",
    "                \n",
    "                # Get ground truth (implementation specific)\n",
    "                lab_true = get_true_lab(batch[i])  \n",
    "                munsell_true = get_munsell_label(batch[i])\n",
    "                \n",
    "                # Calculate metrics\n",
    "                lab_errors.append(np.mean((lab_pred - lab_true)**2))\n",
    "                delta_es.append(self.delta_e(lab_pred, lab_true))\n",
    "                munsell_acc += (self.mapper.lab_to_munsell(lab_pred) == munsell_true)\n",
    "                total +=1\n",
    "                \n",
    "        return {\n",
    "            \"lab_mse\": np.mean(lab_errors),\n",
    "            \"delta_e\": np.mean(delta_es),\n",
    "            \"munsell_acc\": munsell_acc / total\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling LABPreprocessing.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'lab_preprocessing' (of type LABPreprocessing). Either the `LABPreprocessing.call()` method is incorrect, or you need to implement the `LABPreprocessing.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nInput 'y' of 'Sub' Op has type float32 that does not match type float16 of argument 'x'.\u001b[0m\n\nArguments received by LABPreprocessing.call():\n  â¢ args=('<KerasTensor shape=(None, 224, 224, 3), dtype=float16, sparse=False, name=keras_tensor_179>',)\n  â¢ kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Phase 1: General Model \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m general_model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_optimized_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m create_pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/sala/data/general/*.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m general_model\u001b[38;5;241m.\u001b[39mfit(train_ds, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m, in \u001b[0;36mbuild_optimized_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mRescaling(\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)(inputs)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Enhanced preprocessing\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mLABPreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Architecture optimized for mobile\u001b[39;00m\n\u001b[1;32m      9\u001b[0m base \u001b[38;5;241m=\u001b[39m EfficientNetB3(include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     10\u001b[0m                      input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m*\u001b[39mTARGET_SIZE, \u001b[38;5;241m3\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m, in \u001b[0;36mLABPreprocessing.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Normalize with ImageNet stats\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m (\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Convert RGB to LAB using TF ops\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     xyz \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mrgb_to_xyz(x)\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling LABPreprocessing.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'lab_preprocessing' (of type LABPreprocessing). Either the `LABPreprocessing.call()` method is incorrect, or you need to implement the `LABPreprocessing.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nInput 'y' of 'Sub' Op has type float32 that does not match type float16 of argument 'x'.\u001b[0m\n\nArguments received by LABPreprocessing.call():\n  â¢ args=('<KerasTensor shape=(None, 224, 224, 3), dtype=float16, sparse=False, name=keras_tensor_179>',)\n  â¢ kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "# Phase 1: General Model \n",
    "general_model = build_optimized_model()\n",
    "train_ds = create_pipeline(\"/home/sala/data/general/*.jpg\")\n",
    "general_model.fit(train_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Orchestration ###\n",
    "def main():\n",
    "    \n",
    "    \n",
    "    # Phase 2: Self-supervised Fine-tuning\n",
    "    simclr = SimCLR(general_model)\n",
    "    simclr.compile(optimizer=tf.keras.optimizers.Adam(0.0001))\n",
    "    soil_ds = create_pipeline(\"/home/sala/data/soil_data/test/Alluvial soil/*.jpg\")\n",
    "    simclr.fit(soil_ds, epochs=5)\n",
    "    \n",
    "    # Phase 3: Munsell Mapping\n",
    "    mapper = MunsellMapper(\"munsell.csv\")\n",
    "    \n",
    "    # Phase 4: Mobile Conversion\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(simclr.encoder)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = lambda: representative_dataset_gen()\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Save and test\n",
    "    with open(\"soil_color.tflite\", \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    # Evaluate\n",
    "    mobile_model = MobileInference(\"soil_color.tflite\", mapper)\n",
    "    evaluator = ColorEvaluator(mobile_model, mapper)\n",
    "    test_ds = create_pipeline(\"test_images/*.jpg\", shuffle=False)\n",
    "    results = evaluator.evaluate(test_ds)\n",
    "    \n",
    "    print(f\"LAB MSE: {results['lab_mse']:.2f}\")\n",
    "    print(f\"ÎE 2000: {results['delta_e']:.2f}\")\n",
    "    print(f\"Munsell Accuracy: {results['munsell_acc']*100:.1f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
